{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahid-TheMatrixMaker/100-Data-Science-Project/blob/main/Credit%2BCard%2BFraud%2BDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0YV2k1HiZuw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('creditcard.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "5EsZ7tlqkV0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['Class'].value_counts()/len(data))"
      ],
      "metadata": {
        "id": "TYkvyJETiknL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "cdjIegyTiosp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna(axis=0)"
      ],
      "metadata": {
        "id": "yZaA8pGm5yc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = data['Amount'].quantile(0.25)\n",
        "Q3 = data['Amount'].quantile(0.75)\n",
        "IQR = Q3 - Q1"
      ],
      "metadata": {
        "id": "msPgsVUEiuqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers = data[(data['Amount'] < (Q1 - 1.5 * IQR)) | (data['Amount'] > (Q3 + 1.5 * IQR))]"
      ],
      "metadata": {
        "id": "pn-yU_MpixL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Amount'].hist()\n",
        "plt.title('Transaction Amount Distribution')"
      ],
      "metadata": {
        "id": "q1xgE2Eyi3UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x='Amount', y='Time', data=data);"
      ],
      "metadata": {
        "id": "T3ACRh-IjAoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = data.corr()\n",
        "sns.heatmap(corr)"
      ],
      "metadata": {
        "id": "3cmH9ajdjFIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "X = data.drop('Class', axis=1)\n",
        "y = data['Class']\n",
        "oversampler = SMOTE(k_neighbors=1)\n",
        "X_smote, y_smote = oversampler.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "uoAYAllRi18K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_smote, y_smote, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "VdPjpZ5W4PiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=10000)\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "nMDHvO_K4S5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred = clf.predict(X_valid)\n",
        "precision = precision_score(y_valid, y_pred)\n",
        "recall = recall_score(y_valid, y_pred)\n",
        "f1 = f1_score(y_valid, y_pred)"
      ],
      "metadata": {
        "id": "vRboVW6_4cQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Precision:', precision)\n",
        "print('Recall :', recall )\n",
        "print('f1 Score:', f1)"
      ],
      "metadata": {
        "id": "TXKKnr_44WeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Train model 1\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Train model 2\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "# Ensemble predictions\n",
        "y_pred_ensemble = (rf.predict(X_valid) + gb.predict(X_valid)) / 2"
      ],
      "metadata": {
        "id": "UH_5HheSfY4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "anomaly_model = IsolationForest(contamination=0.01)\n",
        "anomaly_model.fit(X_train)\n",
        "\n",
        "anomaly_scores = anomaly_model.decision_function(X_valid)\n",
        "\n",
        "# Define the threshold based on your requirements\n",
        "threshold = -0.5\n",
        "\n",
        "anomalies = anomaly_scores < threshold"
      ],
      "metadata": {
        "id": "qkVOXFPMfZBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "X_train_array = X_train.values\n",
        "X_train_reshaped = X_train_array.reshape(X_train_array.shape[0], X_train_array.shape[1], 1)\n",
        "num_timesteps = X_train_reshaped.shape[1]\n",
        "num_features = X_train_reshaped.shape[2]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(num_timesteps, num_features)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train_reshaped, y_train, epochs=5)"
      ],
      "metadata": {
        "id": "zRgJF_mtr1lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = X_valid.shape[1]\n",
        "input = Input(shape=(num_features,))\n",
        "encoded = Dense(32, activation='relu')(input)\n",
        "decoded = Dense(num_features, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = Model(input, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "recon_err = autoencoder.evaluate(X_valid)\n",
        "anomalies = recon_err > threshold"
      ],
      "metadata": {
        "id": "IZf2kte8vcDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Input\n",
        "\n",
        "# Assuming you have your features in X and labels in y\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the training data\n",
        "X_train_array = X_train.values\n",
        "X_train_reshaped = X_train_array.reshape(X_train_array.shape[0], X_train_array.shape[1], 1)\n",
        "\n",
        "# Reshape the test data\n",
        "X_test_array = X_test.values\n",
        "X_test_reshaped = X_test_array.reshape(X_test_array.shape[0], X_test_array.shape[1], 1)\n",
        "\n",
        "num_timesteps = X_train_reshaped.shape[1]\n",
        "num_features = X_train_reshaped.shape[2]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(num_timesteps, num_features)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model on the training data and validate on the test data\n",
        "model.fit(X_train_reshaped, y_train, epochs=5, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "scores = model.evaluate(X_test_reshaped, y_test, verbose=0)\n",
        "print(f'Test loss: {scores[0]} / Test accuracy: {scores[1]}')\n"
      ],
      "metadata": {
        "id": "-JzkGZP7w2dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = gb.feature_importances_\n",
        "print(importances)"
      ],
      "metadata": {
        "id": "ZJFO9SjJw3BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.barh(X.columns, importances)\n",
        "plt.title('Gradient Boost Feature Importances')"
      ],
      "metadata": {
        "id": "aFksUuaU40R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import pickle\n",
        "from flask import Flask, request, jsonify\n",
        "# Assuming you have your features in X and labels in y\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Reshape the training data\n",
        "X_train_array = X_train.values\n",
        "X_train_reshaped = X_train_array.reshape(X_train_array.shape[0], X_train_array.shape[1], 1)\n",
        "# Reshape the test data\n",
        "X_test_array = X_test.values\n",
        "X_test_reshaped = X_test_array.reshape(X_test_array.shape[0], X_test_array.shape[1], 1)\n",
        "\n",
        "num_timesteps = X_train_reshaped.shape[1]\n",
        "num_features = X_train_reshaped.shape[2]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(num_timesteps, num_features)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model on the training data and validate on the test data\n",
        "model.fit(X_train_reshaped, y_train, epochs=5, validation_data=(X_test_reshaped, y_test))\n",
        "# Evaluating the model\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {scores[0]}')\n",
        "print(f'Test Accuracy: {scores[1]}')"
      ],
      "metadata": {
        "id": "_1jSpXATCuLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the trained model using pickle\n",
        "pickle.dump(model, open('fraud_detection_model.pkl', 'wb'))\n",
        "\n",
        "# Flask application for deployment\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Endpoint for making predictions\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Load the saved model\n",
        "    loaded_model = pickle.load(open('fraud_detection_model.pkl', 'rb'))\n",
        "\n",
        "    # Get the input data from the request\n",
        "    data = request.get_json()\n",
        "    input_data = np.array(data['input'])\n",
        "\n",
        "    # Preprocess the input data as required\n",
        "    input_data = np.reshape(input_data, (1, input_data.shape[0], 1))\n",
        "\n",
        "    # Make predictions using the loaded model\n",
        "    predictions = loaded_model.predict(input_data)\n",
        "\n",
        "    # Return the predictions as a response\n",
        "    return jsonify({'predictions': predictions.tolist()})"
      ],
      "metadata": {
        "id": "LwbTlvSUEEM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the Flask application on a web server\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ],
      "metadata": {
        "id": "-zbOUMUOEGTx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}